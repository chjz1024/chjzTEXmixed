This paper focused on creating a new network for face recognition,  while also sparsified the whole network after the training has finished. What's meaningful to our work is the principle he used to prune weights.

The author tends to keep connections (and the corresponding weights) where neurons connected have high correlations and drop connections between weakly correlated neurons. For fully and locally-connected layers,  where weights are not shared,  given a neuron $a_i$ in the current layer and its K connected neurons $b_{i1} ,  b_{i2} , \ldots , b_{iK}$ in the previous layer,  the correlation coefficient between $a_i$ to each of $b_{ik}$ for $k = 1,  2,  \ldots ,  K$ is
\begin{displaymath}
  r_{ik}=\frac{E[a_i-\mu_{a_i}][b_{ik}-\mu_{b_{ik}}]}{\sigma_{a_i}\sigma_{b_{ik}}}
\end{displaymath}

where $\mu_{a_i}, \mu_{b_{ik}}, \sigma_{a_i}, \sigma_{b_{ik}}$denote the mean and standard deviation of $a_i$ and $b_{ik}$,  respectively,  which are evaluated on a separated training set. Since both positively and negatively neurons are important,  the author used similar but slightly different methods to deal with them. For convolutional layers,  the author calculated the mean magnitude,  which we don't present here.

\textbf{The main insight is to measure weight importance based on the previous layer. The author also experimented on directly training the sparse ConvNet from scratch,  which failed to find good solutions for face recognition. Although we mainly focus on pruning an existing network,  this could be some hint to the training stage.}