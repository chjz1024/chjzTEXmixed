% This paper proposed a filter level pruning framework,  called ThiNet,  to simultaneously accelerate and compress CNN models in both training and inference stages.
该论文提出了filter层面的网络剪枝方案, 被称为ThiNet。使用这种方式可以在训练与推测阶段加速与压缩原网络。

% The idea is to establish filter pruning as an optimization problem,  \textit{i.e.},  to find a subset of channels in layer (i+1)'s (not i's) input to approximate the output in layer i+1. Instead of considering which filter to discard,  the author finds the most important filters to be preserved.
本论文主要思路是将filter剪枝视为一优化问题, 即在i+1层的channel中找到一子集来近似模拟i+1曾的输出。相对于考虑该舍弃哪些filter, 作者考虑的是保留哪些重要filter。

% Given a pre-trained model,  it would be pruned layer by layer with a predefined compression rate. The process is shown below:
该方法对一已训练好的模型根据设定好的压缩率逐层进行进行剪枝, 整个过程示意如下：
\begin{figure}[H]
  \centering
  \includegraphics{subsection/ThiNet.png}
\end{figure}

% \begin{enumerate}
  % \item A training set is randomly sampled.
  % \item Using a greedy algorithm for channel selection,  \textit{i.e.},  to minimize \[\arg\min_T \sum_{i=1}^M \left(\hat{y}_i-\sum_{j\in S}\hat{x}_{i, j} \right)^2\quad s.t.\quad |S|=C\times r,  S\in \{1, 2, \ldots, C\} \]
  % \item Minimize the reconstruction error by weighing the channels,  which can be defined as $\hat{w}=\arg\min_w\sum_{i=1}^M(\hat{y}_i-w^T\hat{x}_i^*)^2$
  % \item Iterate to step 1 to prune the next layer.
% \end{enumerate}
\begin{enumerate}
  \item 随机选出一样本集。
  \item 使用贪婪算法来选择channel, 以最小化 \[\arg\min_T \sum_{i=1}^M \left(\hat{y}_i-\sum_{j\in S}\hat{x}_{i, j} \right)^2\quad s.t.\quad |S|=C\times r,  S\in \{1, 2, \ldots, C\} \]
  \item 对各channel加权来最小化误差, 定义如下 $\hat{w}=\arg\min_w\sum_{i=1}^M(\hat{y}_i-w^T\hat{x}_i^*)^2$
  \item 迭代至步骤一来对下一层剪枝。
\end{enumerate}

作者使用这种方式在VGG-16上以0.52\%的top-5准确度下降换来了$3.31\times$的FLOPs下降以及$16.63\times$的压缩率。

% \textbf{The main insight is that the author establishes a well-defined optimization problem,  which shows that whether a filter can be pruned depends on the outputs of its next layer,  not its own layer. Also because this approach prunes the network in filter level,  the computational cost is relatively low,  compared with weight pruning.}
\textbf{本文主要创新点是作者将剪枝问题视为一优化问题, 其中各filter的重要性以其到下一层的输出来衡量而不是本层。并且由于剪枝是在filter层面进行的, 计算复杂度与权重剪枝相比相对较低。}