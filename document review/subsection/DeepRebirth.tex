This paper finds another solution for speeding up model inference and reducing model size,  which is called DeepRebirth. Strictly speaking this method shouldn't be called pruning,  but it's useful for deployment in mobile phones and has been tested on some mobile devices,  so we list it here.

First the author found that non-tensor layers consume too much time in model execution,  which is shown below:
\begin{figure}[H]
  \centering
  \includegraphics{subsection/DeepRebirth.png}
\end{figure}
So the author thought of combining those non-tensor layers with its adjacent tensor layers. This paper presents two ways to combine those layers,  called \textbf{Streamline Slimming} and \textbf{Branch Slimming} respectively.
\begin{figure}[H]
  \centering
  \begin{minipage}[h]{.49\textwidth}
    \centering
    \includegraphics{subsection/overall.png}
  \end{minipage}
  \begin{minipage}[h]{.49\textwidth}
    \centering
    \includegraphics{subsection/Streamline.png}
  \end{minipage}
\end{figure}
\begin{figure}
  \centering
  \includegraphics{subsection/Branch.png}
\end{figure}

The weights and biases are retrained to attain similar performance with original network,  which is given by
\begin{displaymath}
  (\tilde{W}^*, \tilde{B}^*)=\underset{W, B}{\arg\min}\sum_i ||Y_{CNN}^i-\tilde{f}(W, B;X^i)||_F^2
\end{displaymath}
where $W$ and $B$ represent weight and bias matrix respectively.

\textbf{The main insight in this article is to evaluate execution time of non-tensor layers and solve this problem by merging certain layers,  and it has achieved great success in speeding up the whole network. However,  this is a completely engineering practice. It could be added to any network after the training has finished.}